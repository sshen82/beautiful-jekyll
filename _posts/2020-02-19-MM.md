---
layout: post
title: Majorization-Minimization
gh-repo: daattali/beautiful-jekyll
tags: [Statistics]
comments: true

---

Recently, I encountered a quite interesting non-convex algorithm, so I wrote a general introduction to this. This algorithm is Majorization-Minimization, and I also include the generalized version of this, Generalized Majorization-Minimization. This article is mainly a summary of [1], and [2].

Suppose we want to minimize $$ L(x):R^p \rightarrow R $$, and $$ x^k $$ indicates the current iterate. There are two steps in MM algorithm. 

First is to find a surrogate function: $$ Q(x\|x^k) $$, so that $$ Q(x^k\|x^k) = L(x^k) $$, and $$ Q(x\|x^k) \geq L(x), \forall x $$. In this step, except from the constraints above, the function should also be easy to minimize, and it seems that the popular choice is a quadratic function.

Next, we need to find the minimum $$ x^{k+1} $$ of $$ Q(x\|x^k) $$. It can be seen easily that $$ L(x^{k+1})\leq L(x^k) $$.

The Generalized-MM looses the constraint on $$ Q(x\|x^k) $$, so that instead of being the same value on $$ x^k $$, it is just requiring $$ Q(x^k\|x^{k+1}) \leq Q(x^k\|x^k) $$. Using this constraint, according to the paper, it is better in avoiding local minima.

  When I first hear that it is a "non-convex optimization" algorithm, I thought there is an effective way to get to the global minima, but it seems that this kind of algorithms will still be stuck in local minima. It is trying to avoid this though.




[1] Hunter, David R., and Kenneth Lange. "Quantile regression via an MM algorithm." *Journal of Computational and Graphical Statistics* 9.1 (2000): 60-77.

[2] Parizi, Sobhan Naderi, et al. "Generalized majorization-minimization." *arXiv preprint arXiv:1506.07613* (2015).
